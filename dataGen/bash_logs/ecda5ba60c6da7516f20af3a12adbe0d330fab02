ls
cd apache-mahout-distribution-0.10.1/
ls
cd 
sudo rm -r mahout
sudo mv apache-mahout-distribution-0.10.1 mahout
ls
cd ma
cd mahout/
ls
cd
source /etc/profile
source ~/.bashrc
mahout --help
mahout
ll
mysql -uroot -p
sudo mysql -uroot -p
mysql -uhive -p
R
su air
cd /tmp/RtmpQ0IbCh/downloaded_packages
cd /tmp
ls
ll
echo $SPARK_HOME
cd spark/spark-1.3.0-bin-hadoop2.4/
ls
cd conf
ls
cat slaves
ll
cd ..
ll
cd conf
ls
sudo chmod 755 log4j.properties 
cat spark-env.sh
cd ..
./run-example org.apache.spark.examples.SparkPi local
locate run-example
cd bin 
run-example org.apache.spark.examples.SparkPi local
ls
cd ..
run-example org.apache.spark.examples.SparkPi spark://192.168.1.200:7077
su air1
su air2
ls
sudo service mysql restart
mysql -u mysql -p 
su air
pwd
cd mysql
ls
cd data/
ls
cd test/
ls
cd ..
ls
cpwd
pwd
cd ..
ls
cd ..
ls
cd etc/
ls
cd mysql
ls
vim my.cnf 
sudo vim my.cnf 
souce /etc/profile
source /etc/profile
service mysqld restart
sudo vim my.cnf 
mysql -u root
mysql -u root -p
service 
service mysqld restart
sudo service mysqld restart
sudo service mysql restart
sudo -u root -p
sudo mysql  -u root -p
sudo vim /etc/mysql/my.cnf 
sudo service mysql restart
mysql -u root
mysql -u root -p
mysql -uroot -p
cd /
ls
cd var
ls
cd lib
ls
cd mysql/
sudo cd mysql/
sudo cd mysql
cd mysql/
ls
ls -l
chown -R mysql:mysql /var/lib/mysql/database_name
sudo chown -R mysql:mysql /var/lib/mysql/database_name

sudo chown -R root:root /var/lib/mysql/database_name
sudo chown -R root:root /var/lib/mysql
ls -l
cd mysql/
sudo cd mysql/
cd sudo/
sudo chown -R air:air /var/lib/mysql
ls
ls -l
sudo chown -R root:root /var/lib/mysql
cd mysql 
sudo chown -R mysql:mysql /var/lib/mysql
ll
ls
ls -l
sudo chown -R root:mysql /var/lib/mysql
ls -l
sudo chown -R mysql:mysql /var/lib/mysql
ls -l
ll
ls
hive
ls
hive
hadoop
hadoop fs -ls /user/
cd /home/dmc/hadoop/share
ls
cd hadoop
ls
cd tools
cd lib
ls
cd bin
pwd
cd /home/dmc/hadoop/
ls
cd bin
ls
cd hadoop
cat hadoop
ssh new001
cd /etc/security/
ls
cat limits.
cat limits.d
cat limits.conf 
ulimit -n
cat limits.conf 
ulimit -a
renice +10 `ps aux | awk '{ if ($3 > 0.8 && id -u $1 > 500) print $2}'`
top
ulimit
ulimit --help
ulimit -help
top
sudo vim limits.conf 
cd /etc/pam.d/
ls
sudo vim common-session
sudo vim /etc/profile
cd
jps
hadoop fs -ls /
hadoop version
start-all.sh
hadoop fs -ls /
cd hadoop/
cd bn
cd bin
ls
cd ha
cd hadoop
ll
cat hadoop
vim  hadoop
java -version
hadoop version
cd ..
cd share/
cd hadoop/
ll
cd tools/lib/
ls
start-all.sh
hive
cd ..
cd 
cd hadoop/bin/hdfs
cd hadoop/bin
ls
cat hdfs
cd
ls
sudo scp -r hdfs hadoop/bin/
start-all.sh
jps
hadoop fs -ls /
ls
top
cd /home
ls
cd mobile3
ls
ls -l
ls
mkdir s
sudo mkdir s
ls
rm -r s
sudo rm -r s
ls
hive
mahout
mahout trainlogistic --input donut.csv  --output ./model  --target color  --categories 2  --predictors x y  --types numeric --features 20  --passes 100 --rate 50  ##Ã¨Â¿ÂÃ¨Â¡Å’Ã¦Â¨Â¡Ã¥Å¾â€¹
ls
cd /
ls
mysql -uroot -p
su air
spark
pyspark
spark
jps
spark
pyspark
hadoop fs -ls /
start-all.sh
locate hadoop-env.sh
cd /etc/hadoop/hadoop-env.sh
cd hadoop/
cd /etc/hadoop/hadoop-env.sh
cd etc/hadoop/
ls
cat hadoop-env.sh
echo $HADOOP_SECURE_DN_USER
hadoop fs -ls /
ls
cat hadoop-env.sh
echo $JAVA_HOME
sudo vim hadoop-env.sh
source /etc/profile
start-all.sh
hadoop fs -ls  /
hive
mysql -uroot -p
ls
sudo mkdir mysql
ls
ll
cd mysql 
ls
rpm -qa|grep mysql
sudo apt-get install rpm
sudo rpm -e --nodeps mysql-libs-5.1.71-1.el6.x86_64
cd
cd hive
ls
rz
ls
cd data
ls
cd data1
ls
cd ..
cd data2
ls
cd 
ls
sudo mv data exercise
ls
ll
sudo chmod -r 774 /exercise
sudo chmod -r 774 exercise
sudo chmod  774 exercise
ll
ls
ping www.baidu.com
clear
start-all.sh
jps
hadoop fs -ls /
lastlog
ls
mahout
spark
clear
fdisk -l
fdisk -l more /proc/partions
df -h
+
fdisk -l 
cd /tmp
ls
sudo mkdir data
sudo mount /dev/sde1 /tmp/data
ls
cd data
ls
uuu    
sudo umount /tmp/data
cd
sudo umount /tmp/data
ls
cd /tmp
ls

sudo mount /dev/sde1 /tmp/data
ls
cd data
ls
sudo scp -r data /home/dmc
ls
cd
sudo umount /tmp/data
cd /tmp/data
ls
clear
cd
clear
~
MYSQL
mysql
ADMC123
mysql -UROOT -P
mysql -uroot -p
ls
mysql -uhive -p
mysql -uroot -p
mysql -uroot -pmysql
ll
top
ls
ll
htop
free -m
hive
sudo /etc/profile
ls
start-all.sh
jps
hadoop fs -ls /
hive
sudo -uhive -p
sudo -uroot -p 
mysql -uroot -p 
mysql -umysql -p
mysql -uhive -p
hadoop fs -ls /
cd /home/dmc/hadoop/share/hadoop/tools/lib/
ls
hadoop version
java version
echo JAVA_HOME
echo $JAVA_HOME
cat /etc/profile
top
cd
jps
df -h
df -h /home
sudo apt-get install quota  
sudo apt-get install samba-common
sudo apt-get install smbclient
cd /usr/bin
ll
ll d*
sudo apt-get install smbclient
cd dkpg
cd dpkg
cd /var/lib/dpkg/
ll
sudo mv info info.bak
sudo mv info info.bak.1
sudo mkdir info
sudo apt-get install quota  
cd /etc/fstab
sudo vim /etc/fstab
df -h /home

ls
cd /etc/
ls
cat fstab.d/
cd fstab.d/
ls
ll
cd ..
sudo cp fstab fstab.old
ls
sudo vim fstab
df -h /home
sudo vim fstab
top
cd /home
ll
df -h /home/
cd air3
ll
cd ..
cd air
ll
top
sudo rebot
sudo reboot
ping www.baidu.com
if config eth0 211.71.20.244 up; ,;  dsjcioj; 
ping www.baidu.com
route add default gw 211.71.20.193
cat /etc/resolv.conf 
service network restart
service ssh status 
service sshd start
sudo apt-get update
R
ping www.baidu.com
cat /etc/issue
ls
ll
ping www.baidu.com
cd /etc/network/
ls
cd interfaces.d/
cd ..
sudo vim interfaces
sudo reboot
sudo apt-get update
sudo apt-get install r-base
cd /var/lib/dpkg/
ls
cd info
ls
sudo rm -f  foomatic-filters
LS
ll
sudo rm -f  foomatic-filters
ls
sudo apt-get update -f
cd ..
ls
sudo mv info info.bak.2
sudo mkdir info
sudo apt-get install r-base
R
ls
ping www.baidu.com
ll -d /a*
quotaon
lastlog
edquota -u ruc15 
cat /etc/fstab
edquota -u
edquota -u air1
edquota -u air1(uid 1009)
sudo edquota -u air1(uid 1009)
sudo edquota -u air1
quota -gvs 
sudo edquota -u air1
sudo vim  edquota -u air1
sudo   edquota -u air1
edquota -p air1 -u air2
sudo edquota -p air1 -u air2
sudo edquota -p air1 -u air3
sudo edquota -p air1 -u air4
sudo edquota -p air1 -u air5
lastlog
sudo edquota -p air3
sudo edquota -p air2
sudo edquota -u air2
sudo edquota -u air3
sudo edquota -p air1 -u mobile1
sudo edquota -p air1 -u mobile2
sudo edquota -p air1 -u mobile3
sudo edquota -p air1 -u mobile4
sudo edquota -p air1 -u mobile5
sudo edquota -p air1 -u ruc15
sudo edquota -p air1 -u pku15
sudo edquota -p air1 -u cufe15
sudo edquota -p air1 -u cueb15
sudo edquota -p air1 -u ustc15
sudo edquota -p air1 -u bjut15
edquota -t
QUOTA -U
quota -u
quota -u air1
sudo quota -uvs air1 air2
sudo quota -u air2
quotaon
quotaon -avug
sudo quotaon -avug
renice +10 `ps aux | awk '{ if ($3 > 0.8 && id -u $1 > 500) print $2}'` 
ping www.baidu.com
R
top
df -i
top
su air4
ls
edquota -u air1 
sudo edquota -u air1 
repquota -a
sudo repquota -a
edquota -p air1 -u air2
sudo edquota -p air1 -u air2
sudo edquota -p air1 -u air3
sudo edquota -p air1 -u air4
sudo edquota -p air1 -u air5
sudo edquota -p air1 -u mobile1
sudo edquota -p air1 -u mobile2
sudo edquota -p air1 -u mobile3
sudo edquota -p air1 -u mobile4
sudo edquota -p air1 -u ruc15
sudo edquota -p air1 -u pku15
sudo edquota -p air1 -u cufe15
sudo edquota -p air1 -u cueb15
sudo edquota -p air1 -u ustc15
sudo edquota -p air1 -u bjut15
sudo repquota -a
sudo apt-get install r-base r-base-dev
R
IPYTHON
ipython
su air1
su air2
hive
echo $HIVE_HOME
LL
ll
groups air1
su air1
su air2
ls
top
ls
cd /home
ls
cd air2
ls
cd R
ls
cd //
ls
free -m
ls
bin
ls
hive
top
mysql -uroot -p
ls
cd /home/air/wgb
sudo cd /home/air/wgb
ls
cd /home/air/wgb
cd /home
cd air
ls
cd wgb/
sudo cd wgb/
ls
top
ls
free
jps
pyspark
jps
start-all.sh
jps
start-all.sh
su new002
ssh  new002
jps
ls
ls /home
cd  /
ls
sudo rm -r hadoop-native-64-2.6.0.tar 
ls
mysql -uroot -p
cd  /home/air/wgb/standalone
sudo cd  /home/air/wgb/standalone
sudo 
su dmc
ls
cd /home/air/wgb/standalone
sudo cd /home/air/wgb/standalone
sudo /home/air/wgb/standalone
sudo cd /home/air/wgb/standalone
echo $PATH
ls
mkdir /home/air1/wgb
sudo mkdir /home/air1/wgb
cd /home/air1/wgb
sudo cd /home/air1/wgb
sudo cd /home
cd /home/air
ls
cd wgb/
sudo cd wgb/
ls
cd amy/
ls
cd hjq
cd ..
ls
cd hjq/
ls
cd select_from_hive/
ls
wq
hive 
ls
sudo cp  mydata2.csv  /home/air1/wgb
ls
sudo cp  result2.txt  /home/air1/wgb
LS
ls
top
ls
cd project/
ls
ll
cd airdata/
ls
ll
cd ..
ll
R
su root
cd hive
ls
ll
ls
locate iotmp
chmod 755 iotmp
sudo chmod 755 iotmp
cd 
cd /home/dmc/hive/iotmp
locate iotmp
pwd
cd hive
ls
locate iotmp
ls
cd 
ll
cd hive
cd 
cd hive1.1/
ls
cd 
cd hive
sudo mkdir iotmp
ll
sudo chown dmc:hadoop iotmp
ll
hive
cd ..
cd hadoop/
ll
cd map.R
cat map.R 
cd tmp
ls
cd dfs
ls
ll
cd data
ls
cd ..
ll
cd name
ll
cd current/
cd ..
ls
cd ..
ls
cd java/
ls
ll
cd /home/dmc/hive/iotmp
cd /home/dmc/hive/
ls
sudo mkdir iotmp
ls
ll
sudo chown dmc:hadoop iotmp
ll
ls
cd iotmp
ls
hive
LS
ls
cd ..
ls
ll
cd conf
ls
cat hive-env.sh 
ls
cat hive-config.sh 
locate hive-site.xml 
cat hive
cat hive-site.xml 
sudo vim hive-site.xml 
cd 
cd hive
ls
sudo mkdir iotmp
ls
ll
sudo chown dmc:hadoop iotmp
ll
sudo chmod 777 iotmp
ll
cd
ll
ls
mkdir software
ls
sudo cp apache-maven-3.3.9-bin.tar.gz software
sudo mv apache-maven-3.3.9-bin.tar.gz software
sudo mv mysql-5.5.47-linux2.6-x86_64 software/
cd software/
ls
cd 
sudo mv MySQL-5.5.47-1.linux2.6.x86_64.rpm-bundle.tar software/
sudo mv hadoop-native-64-2.6.0.tar software/
sudo mv mahout-collections-1.0-bin.zip software/
sudo mv mysql-connector-java-5.1.* software/
sudo mv apache-hive-1.* software/
ls
sudo mv apache-mahout-distribution-0.10.1.tar.gz  software/
sudo mv cmake-3.4.1.tar.gz 
sudo mv cmake-3.4.1.tar.gz  software/
ls
sudo mv mysql-5.5.47-linux2.6-x86_64.tar.gz software/
rm -r 001.txt 
cd configuration/
ls
cat bashrc 
cd 
ls
rm -r configuration/
sudo rm -r configuration/
ls
sudo rm -r authorized_keys 
ls
ll
cd hdfs
cat hdfs
ls
sudo rm -r hdfs 
ls
sudo rm -r bashrc 
l
sudo rm -r derby.log 
ls
cat /etc/profile
ls
cd project/
ls
cd 
cd pelican/
ls
cd bin
ls
cd
ll
cd setupfile/
ls
cd 
sudo mv setupfile/ software/
ls
cd lrq
ls
cd native
ls
cd 
ls
sudo mv lrq/ software/
ls
cd wxn
ls
cd
sudo rm -r wxn
ls
sudo rm -r hive1.1/
ls
R
ssh dmc@211.71.20.245
ls
cd /home/air1
su air1
mysql -uroot -pdmc1056
cd hive
cd conf
sudo vim hive-site.xml 
top
hadoop fs -ls /
groups mobile
hadoop fs -ls /user/hive
hadoop fs chmod 775 /user/hive
sudo hadoop fs chmod 775 /user/hive
sudo hadoop fs -chmod 775 /user/hive
sudo hadoop fs chmod -R 775 /user/hive
sudo hadoop chmod -R 775 /user/hive
hadoop fs -chmod -R 775 /user/hive
hadoop fs -ls /
hadoop fs -ls /user
cd hive
ls
cd conf
ls
sudo vim hive-site.xml 
su mobile
hive
su air
su mobile
hive
cd hive
cd conf/
hive
sudo vim hive-site.xml 
hive
su root
hive
ls
hive
ls
hive
ls
hive
show tables;
hive
free -m
cat /etc/security/limits.conf 
sudo vim /etc/security/limits.conf 
ulimit -s unlimited
free -m
su root
cd hive
ls
mkdir iotmp
ls
chmod 777 iotmp/
ll
ll
cd hivw
cd hive
sudo chmod 766 iotmp/
ls
ll
cd hive
ls
ll
su root
cd hive
ls
sudo chmod 777 iotmp/
su root
hive
su root
hadoop fs -chmod -R 777 /tmp
su root
cd hive
ll
sudo chmod 666 iotmp/
ll
cd iotmp/
ll
hive
sudo chmod 776 iotmp/
hive
su root
top
hive
ls
hive
top
ls
mahout
mahout testforest
top
free -m
top
ls
cd project
ls
cp /home/air1/hjq/weather_all_formatted_complete.csv  /home/air/wgb/weatherdata/rawweatherdata.csv
sudo cp /home/air1/hjq/weather_all_formatted_complete.csv  /home/air/wgb/weatherdata/rawweatherdata.csv
su air1
mysql -uroot -p
find / -name echo.txt
ls
cd ..
ls
cd ..
su air
R
cd /usr/local/
ls
cd lib/
cd R
ls
cd library/
ll
locate randomForest
cd
ls
cd R
ls
cd x86_64-unknown-linux-gnu-library/
ls
cd 3.2
ls
su root
R
ls
pwd
cd ..
ls
top
su root
hadoop fs -ls /tmp
hadoop fs -ls /tmp/hive
hadoop fs -ls /tmp/hive/air
hadoop fs -ls /tmp/hive/air/3d*
hadoop fs -ls /tmp/hive/mobile
hadoop fs -ls /tmp/hive/
hadoop fs -ls /tmp/hive/mobile
hive
rz
ls
pwd
vim wenyi.csv 
rz 
ls
pwd
mahout buildforest --data /home/dmc   --dataset train0120.csv   --nbtrees 10 --output /home/dmc/mahout_file2
su root
su root
ls
jps
ls
dfs -ls /user/hive/warehouse
hadoop dfs -ls /user/hive/warehouse
hadoop dfs -ls /user/hive/warehouse/airdata/
hadoop dfs -ls /user/hive/warehouse/airdata.db
hadoop dfs -ls /user/hive/warehouse/airdata.db/wenyi
hadoop dfs -ls /user/hive/warehouse/airdata.db/wenyi/wenyi.csv
hadoop dfs -cat /user/hive/warehouse/airdata.db/wenyi/wenyi.csv
hive
hivehive
ckear
clear
hive
hadoop fs -ls /user/hive/
hadoop fs -ls /user/hive/warehouse
hadoop fs -ls /user/hive/warehouse/airdata.db
ls
hive
hive
ls
rm tmp.txt 
ls
sz wenyi.csv 
mkdir tmp
cd tmp/
ls
rz 
vim wenyi.csv 
cd ..
ls
vim wenyi.csv 
cd tmp/
ls
pwd
ls
rm wenyi.csv 
ls
rz 
vim wenyi.csv 
hive
ls
vim wenyi.csv 
vim tmp/
cd tmp/
ls
vim wenyi.csv 
pw
pwd
ls
ls
rm train0120.csv 
ls
vim wenyi.csv 
hive
hive -S -e "select * from airdata.wenyitable limit 10;" > tmp.txt
ls
ls -l
vim tmp.txt 
echo $LANG
sudo vi /etc/sysconfig/i18n
sudo vim  /etc/sysconfig/i18n
cd /etc
ls
cd sysconfin
cd sysconfi
top
su root
ls
sudo rm -r wenyi.csv 
ls
rz
ls
pwd
cat wenyi.csv 
head wenyi.csv
clearec
echo Ã¯Â¿Â¥LANG
echo $LLANG
echo $LANG
hive
su root
ls
top
sudo vim shadow
groups
touch ss.txt
touch ss
top
vim /etc/group
ls
vim /etc/passwd
ls
useradd
sudo useradd -d /home/wanggaobin  -s /bin/bash -m  wanggaobin
sudo passwd wanggaobin 
ls
cd /home
ls
cd ..
ls
ls r*
ls
cd root/
sudo cd root/
cd root/
sudo cd root/
sudo -cd root/
sudo ls root/
cd root/

cd  /home
ls
cd air
ls
sudo vim /etc/group
ls
cd /etc
ls
sudo vim passwd
sudo vim group
echo $PATH
cd /home
ls
cd dmc
ls -l
group
groups
su wanggaobin
hadoop
hadoop fs -ls 
hadoop
ls
cd wgb/
cd amy/
ls
su wanggaobin
ls
vim /etc/sudoers
sudo vim /etc/sudoers
hadoop
sudo vim /etc/group
sudo vim /etc/passwd
sudo vim /etc/group
ls
cd /mnt
ls
cd data
ll
cd 
ll
cd pro
cd project/
ll
ls
df -h
cd mobiledata/
ls
cd ..
ls
cd airdata/
ll
ls
cd /air
cd /home
cd air
ls
cd wgb
su root
top
sudo /mnt
cd /mnt
cd data/project/
ls
cd aiadata
cd airadata
cd airdata
ls
cd ..
cd mobiledata/
ls
cd ..
ls
cd ..
ll
cd project
du -sh
df -h 
ls
cd hive/
cd conf
ls
sudo vim hive-site.xml 
hive
su root
cd hive
ls
cd conf
ls
sudo vim hive-site.xml 
top
hive
ntpdate asia.pool.ntp.org
sudo ntpdate asia.pool.ntp.org
cd hive
ls
cd conf
ls
sudo vim hive-site.xml 
start-all.sh
ssh new007
ssh new001
stop-all.sh
cd
start-all.sh
jps
start-all.sh
hive
hive
cd hive
cd conf/
ls
cd 
ls
cd software/
ls
tar -zxvf apache-hive-1.0.1-bin.tar.gz 
ls
cd apache-hive-1.0.1-bin/
;s
ls
cd conf
ls
sudo vim hive-default.xml.template 
cd
hive
cd ive
cd hive
ls
mkdir iotmp
ls
hive
cd conf
ls
sudo vim hive-site.xml 
cd 
cd hadoop/
ls
cd conf
cd etc/
cd hadoop/
ls
sudo vim mapred-site.xml
sudo vim core-site.xml 
cd ..
ll
cd etc
cd hadoop/
ls
cat mapred-site.xml
cd..
cd ..
ls
cd ..
ls 
ll
sudo chmod 777 tmp
ls
cd 
cd hive
ls
sudo mkdir iotmp
ls
sudo chmod 777 iotmp/
ls
route add default gw 211.71.20.193
ping 211.71.20.244
ping 202.112.112.100
service network restart
ping www.baidu.com
clear
start-all.sh
hadoop fs -ls /
hive
top
++++++++++++++++++++++
top
z
start-all.sh
ls
cd hive
ll
ls
top
~~~~
quot	
df -h
df -h 
free -m
df -h
cd /mnt
ls
mkdir data
sudo mkdir data
ls
sudo mount /dev/sda1 /mnt/data
ls
fdisk -l
fdisk -l /more/proc/partions
sudo mount /dev/sdc /mnt/data
df -h 
sudo mount /dev/sde /mnt/data
free -m 
df -h 
df -h
sudo mount /dev/sde5 /mnt/data
sudo  /mnt/data
mnt/data
fdisk -l
ubuntu 
sudo fdisk -l
sudo mount /dev/sde1 /mnt/data
ls
cd data
ls
cd
ls
cd project/
ls
cd air
cd airdata/
ls
cd 
sudo cp -r project /mnt/data
cd /mnt/data
ls
cd project
ls
du -sh
sudo umount /mnt/data
cd
clear
[B[B[B[B
clear
handoop
hadoop fs -ls /
top
`



'''


ls
sudo apt-get update
ping www.baidu.com
sudo route add default gw 211.71.20.193
ping www.baidu.com
ifconfig
netstat -n
reboot now
sudo reboot now
ls
sudo adduser bike
su bike
ls
lastlog
userdel -r ruc15
sudo userdel -r ruc15
top
userdel -r air1
lastlog
sudo userdel -r mobile1
lastlog
sudo userdel -r mobile2
sudo userdel -r mobile3
sudo userdel -r mobile4
lastlog
sudo userdel -r air1
sudo userdel -r air2
sudo userdel -r air3
sudo userdel -r air4
lastlog
sudo adduser -r ruc15 
sudo useradd -r ruc15 
lastlog
userdel -r ruc15
sudo userdel -r ruc15
sudo useradd -m ruc15
sudo userdel ruc15
ls
lastlog
sudo adduser ruc15
groups
groups air
sudo usermod -a -G hadoop bike
groups bike
su bike
ls
pwd
ls -l
cd project/
ls
ls -l
mkdir bicycledata
ls -l
chmod 777 bicycledata/
ls -l
pwd
cd bicycledata/
ls
pwd
ls
ls -l
pwd
ls
mkdir trandata
mv ./*.csv  ./trandata/
ls
cd trandata/
ls
ls -l
cd  ..
ls
mkdir sxjdata
ls
cd sxjdata/
ls
sudo rz
ls
cd 
ls
cd pro
cd project/
ls
bicycledata/
ls
cd bicycledata/
ls
mv  ./*.csv  ./sxjdata/
ls
cd sxjdata/
ls
ls -l
cd ..
ls
ls -l
chmod 741 bicycledata/
ls -l
chmod 774 bicycledata/
ls -l
chmod 754 bicycledata/
ls -l
chmod -R  754 bicycledata/
sudo chmod -R  754 bicycledata
ls -l
cd bicycledata/
ls
ls -l
ls
pwd
cd pro
cd project/
ls
cd bicycledata/
ls
mkdir files
cd files
ls
sudo rz 
ls
sudo rz 
ls
sudo rz 
ls
cd ..
ls
vim readme.txt
ls
ls -l
cat readme.txt 
ls
cd sxjdata/
ls
cd 
ls
cd project/
ls
cd bicycledata/
ls
ls -l
ls
cd ..
ls
pwd
cd bicycledata/
ls
pwd
ls 
ls -l
ls
cd project/
ls
cd bicycledata/
ls
mkdir weatherdata
ls -l
cd weatherdata/
ls
sudo rz 
ls
head weatherdata_hz.csv 
ls -l
ls
cd py
ls
cd exer
cd exercise/
ls
cd ..
sudo rm -r exercise/
ls
rm -r timezone_test/
sudo rm -r timezone_test/
ls
sudo rm -r standalone/
l
sudo rm -r weatherdata/
ls
cd 
gz
rz
sz
ls
sudo rm -r py
ssh dmc002
cd /etc/
ls
cat slaves
cat hostname
cat hosts
ssh new001
ls
sudo chmod 777 wgb
ls
sudo mv wgb py
ls
cd py
ls
ll
rm -r
sudo rm -r airdata
ls
cd rm -r airdata_temp/
ls
sudo rm -r airdata_temp/
cd output/
ls
cd ..
sudo rm -r output/
ls
sudo rm -r cluster/
cd standalone/
ls
cat l.R 
cat weather.R 
su root
lastlog
ls
cd'
csd








cd hive/
ls
mkdir iotmp
ls
sudo chmod 774 iotmp
ls
ll
su root
sudo route add default gw 211.71.20.243
ping www.baidu.com
jps
start-all.sh
hive
hadoop fs -ls
hadoop fs -ls /
hive
su air
su root
lastlog
top
exit
lastlog
sudo useradd lvxiaoling
lastlog
sudo userdel lvxiaoling
sudo adduser lvxiaoling
sudo gpasswd -a lvxiaoling hadoop
su lvxiaoling 
sudo apt-get install pyspark.milib.recommendation
spark
su root
scala
spark version
hadoop version
LASTLOG
lastlog
rz
ls
cd 
sudo userdel ruc15
latlog
lastlog
sudo userdel ruc15
lastlog
sudo userdel ruc15
sudo userdel ruc15
lastlog
sudo adduser ruc15
su ruc15
ls
cd 
ls
cd file
ls
cat /etc/profile
ls
sudo tar zxvf spark-1.6.0-bin-hadoop2.6.tgz 
ls
sudo vim /etc/profile
source /etc/profile
cd spark-1.6.0-bin-hadoop2.6/
ls
cd bin
ls
cd ..
cd conf/
ls
cp spark-env.sh.template spark-env.sh
sudo cp spark-env.sh.template spark-env.sh
cp spark-env.sh.template spark-env.sh
sudo vim  spark-env.sh
sudo cp slaves.template slaves
sudo vim slaves
cdÃ£â‚¬â€šÃ£â‚¬â€š
cd ..
ls
./start-all.sh
cd bin
ls
cd ..
ls
cd sbin
ls
start-all.sh
pyspark
spark-shell
ls
cd hadoop/
ls
cd ..
cd spark/
ls
cd spark-1.3.0-bin-hadoop2.4/
ls
cat /etc/profile
ls
cd conf
ls
cat spark-
cat spark-env.sh
cat slaves
cd
sudo vim readme.txt
ls
cd
sudo chown -R -v ruc15:hadoop /home/ruc15
su ruc15
cd file
rz
ls
ll
sudo tar zxvf spark-1.6.0-bin-hadoop2.4.tgz 
sudo vim /etc/profile
ls
cd spark-1.6.0-bin-hadoop2.6/conf/
ls
sudo cp -r spark-env.sh /home/dmc/file/spark-1.6.0-bin-hadoop2.4/conf/
sudo cp -r slaves /home/dmc/file/spark-1.6.0-bin-hadoop2.4/conf/
cd ..
cd spark-1.6.0-bin-hadoop2.4
cd conf
ls
cat slaves
cat spark-env.sh
cd ..
ls
cd sbin/
ls
stop-all.sh
start-all.sh
pyspark
cd 
file
cd file
ls
sudo rm -r spark-1.6.0-bin-hadoop2.6
ls
cd
ls
cp file/spark-1.6.0-bin-hadoop2.4.tgz software/
cd software/
ll
cd
cd file
ls
sudo rm -r spark-1.6.0-bin-hadoop2.4.tgz 
sudo rm -r spark-1.6.0-bin-hadoop2.6.tgz 
ls
start-all.sh
cd /home/mobile
cd sujianan
cd recspark
/home/dmc/file/spark-1.6.0-bin-hadoop2.6/bin/pyspark recspark.py
cd ..
cd file
ls
cd ..
cd .
cd /home
ls
cd file
ls
/home/dmc/file/spark-1.6.0-bin-hadoop2.6/bin/pyspark recspark.py
cd ..
cd /home/mobile/sujianan/recspark
ls
/home/dmc/file/spark-1.6.0-bin-hadoop2.4/bin/pyspark recspark.py
start-all.sh
su root 
lastlog
su bike
jps
ssh nsrc@211.71.211.124
logout
ifconfig
cat /etc/hosts
ifconfig
ssh new010
java -version
echo $JAVA_HOME
cd hadoop
ls
cd java
ls
ll
ifconfig
netstat -rn
cat /etc/network/interfaces
sz hadoop_old/
**01000000039a32ÂÅ 
sz hadoop_old
**01000000039a32ÂÅ 
sudo chmod 777 hadoop_old
ls
sz hadoop_old/
**01000000039a32ÂÅ 
sz hadoop_old
**01000000039a32ÂÅ 
cd hadoop_old
ls
ll
sz etc
**01000000039a32ÂÅ 
cd hadoop_old
ls
chmod 777 etc
sudo chmod 777 etc
ls
sz etc
**01000000039a32ÂÅ 
cd etc
ls
cd hadoop/
ls
sz core-site.xml 
sz hdfs-site.xml 
sz mapred-
sz mapred-site.xml
sz yarn-site.xml 
cat slaves
cat /etc/profile
sudo scp  /etc/profile /
ls
sudo scp  /etc/profile /home/nsrc
ls
sudo scp  /etc/profile /home/dmc
ls
cd /
ls
sudo rm -r profile
ls
cd /home
ls
sudo rm -r nsrc
ls
cd 
ls 
cd hadoop/
ls
cd java/
ls
cd
ls
cd software/
ls
ssh new001
ls
cd setupfile/
ls
java -version
sz 
sz jdk-7u79-linux-x64.gz 
sz jdk-7u45-linux-x64.tar.gz 
cd
cd hadoop/
ls
cd etc/hadoop/
ls
cat core-site.xml 
cd
sz hadoop
**01000000039a32ÂÅ 
sudo cp hadoop/ hadoop_old
ls

sudo cp hadoop hadoop_old
sudo cp  -r hadoop hadoop_old
ls
start-all.sh
jps
R
LASTLOG
lastlog
ls
sudo rm -r hadoop
hadoop fs -ls /
cd hadoop_old
ls
sudo hadoop_odl
cd 
cd scp -r hadoop_old hadoop 
sudo  scp -r hadoop_old hadoop 
ls
start-all.sh
jps
cd hadoop
ls
ll
cd
sudo chown dmc:hadoop hadoop
ll
cd hadoop
ll
sudo chown dmc:hadoop hadoopcd
cd
sudo chown -R dmc:hadoop hadoop
LL
ll
ls
cd hadoop
ll
start-all.sh
jps
hive
cd hive
cd /tmp
ll
sudo chown -R dmc:hadoop /tmp/hive
hive
hdfs dfsadmin -safemode leave
hive
echo $HIVE_HOME
sudo scp -R hive hive_old
sudo scp -r hive hive_old
ls
cd
sudo scp -r hive hive_old
ls
sz hive_old
**01000000039a32ÂÅ 
ls
echo $SPARK_HOME
cd file
cd spark-1.6.0-bin-hadoop2.4/
ls
cd conf
ls
sz spark-env.sh
cat slaves
cd
ls
cd software/
ls
cd setupfile/
ls
hive
cd hive
ls
sudo mkdir iotmp
sudo rm -r iotmp
ls
sudo mkdir iotmp
hive
java
java -version
hive
cd /usr/myhome
cd /usr
ls
ll
whereis upload
locate upload
hive
cd 
cd hive
ls
sudo chown 776 iotmp/
ll
sudo chown -R dmc:hadoop iotmp
hive
su root
ls
cd hive
ls
ll
sudo chmod 764 iotmp
hive
ll
sudo chmod 774 iotmp
sudo gpasswd -a wanggaobin hadoop
su root 
su root
cd cdÃ©â€¢Â¿cd 
cd
cd hive
ls
mkdir iotmp
ll
sudo chown  666 iotmp
hive
sudo chown  776 iotmp
hive
ls
cd /home/wanggaobin
sudo cd /home/wanggaobin
/home/wanggaobin/code/ProjectBicycle/QueryData
hive -f /home/wanggaobin/code/ProjectBicycle/QueryData/gif1.hql > /home/wanggaobin/code/ProjectBicycle/EDA/gif1.txt
sudo hive -f /home/wanggaobin/code/ProjectBicycle/QueryData/gif1.hql > /home/wanggaobin/code/ProjectBicycle/EDA/gif1.txt
sudo hive -f /home/wanggaobin/code/ProjectBicycle/QueryData/gif1.hql > result.txt
hive -f /home/wanggaobin/code/ProjectBicycle/QueryData/gif1.hql > result.txt
sudo cp /home/wanggaobin/code/ProjectBicycle/QueryData/gif1.hql  hqlq.hql
ls
vim result.txt 
ls
hive -f hqlq.hql  > result.txt
vim hqlq.hql 
sudo vim hqlq.hql 
hive -f hqlq.hql  > result.txt
sudo vim hqlq.hql 
hive -f hqlq.hql  > result.txt
ls
wc result.txt 
head result.txt 
sudo sz result.txt 
ls
cat hqlq.hql 
hvie
hive
ls
touch hql2.hql
vim hql2.hql 
sudo vim hql2.hql 
hive -f hql2.hql 
sudo vim hql2.hql 
hive -f hql2.hql 
sudo vim hql2.hql 
ls
ls -al
start-all.sh
jps
echo $HADOOP_HOME
cd hadoop
ls
cd etc/hadoop/
ls
cat core-site.xml 
cd ..
ls
cd tmp
ls
cd ..
sudo mv tmp/
sudo mv tmp tmp_old
mkdir tmp
ll
start-all.sh
jps
stop-all.sh
999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999
jps
start-all.sh
jps
ls
sudo scp -r /home/dmc/hadoop_old/tmp/ /home/dmc/hadoop
ls
cd tmp
ls
ll
sudo chown -R dmc:hadoop /home/dmc/hadoop/tmp
ll
stop-all.sh
start-all.sh
jps
hadoop fs -ls /
hive
cd
cd hive
ls
mkdir iotmp
ls
cd
ls
ll
cd hive
ll
mkdir iotmp
jps
hive
hadoop dfsadmin -safemode leave
hive
